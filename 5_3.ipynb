{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN97G8qSaaCPjJSibbQ9FHr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/odesay97/MachineLearing_Class/blob/main/5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl4V1GsHGV7S",
        "outputId": "f9c4fdc0-0ba9-4b24-951e-e53df54f9d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n",
            "[0.23167441 0.50039841 0.26792718]\n",
            "0.8934000384837406\n",
            "0.9974503966084433 0.8887848893166506\n",
            "[0.20183568 0.52242907 0.27573525]\n",
            "0.8881086892152563 0.8720430147331015\n",
            "0.9464595437171814 0.8780082549788999\n",
            "[0.15872278 0.68010884 0.16116839]\n",
            "0.9321723946453317 0.8801241948619236\n",
            "[0.08876275 0.23438522 0.08027708]\n",
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 트리의 앙상블\n",
        "\n",
        "# 정형데이터 vs 비정형데이터\n",
        "# 정형데이터는 csv파일처럼 구조를 가지고 있는데이터\n",
        "# 비정형데이터는 택스트데이터, 사진, 디지털 음악등 구조를 가지고 있지 않은 데이터\n",
        "\n",
        "#--> k최근접이웃, 선형회귀, 릿지,라쏘, 다항회귀,로지스틱회귀, 결정트리 등\n",
        "# 지금까지 배운 방식들은 정형데이터에 적합한 방식임\n",
        "\n",
        "### 앙상블 학습\n",
        " # 정형 데이터를 다루는데 가장 뛰어난 방식\n",
        "\n",
        "### 랜덤 포레스트\n",
        "# 앙상블 학습 중 가장 유명하고 안정적인 성능을 제공\n",
        "\n",
        "# 1. 훈련하기 위한 데이터를 랜덤하게 만듬\n",
        "#    입력한 훈련 데이터에서 랜덤하게 샘플을 추출함\n",
        "#    중복된 샘플을 추출할 수 있음\n",
        "#    이렇게 만들어진 샘플을 부트스트랩 샘플이라함\n",
        "#    기본적으로 훈련세트의 크기와 동일하게 설정됨\n",
        "# 샘플생성 랜덤 + 모델생성 랜덤 + 특성선택 랜덤\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "target = wine['class'].to_numpy()\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 랜덤포레스트 사용 -> 앙상블패키지\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "# 특성 중요도 -> 특성 중요도가 좀더 평균적이 됨 ( 2번쨰 특성의 중요도가 낮아짐 )\n",
        "# 왜? -> 랜덤포레스트에서 특성집합에서 루트(특성개수)만큼 랜덤 선택해서 분할선택하는 방식이 있기 때문에\n",
        "# 각특성이 골고루 영양력을 받음\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.feature_importances_)\n",
        "\n",
        "\n",
        "# OOB -> 랜덤 과정에서 선택되지 않은 샘플 -> 나중에 테스트샘플로 씀\n",
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(rf.oob_score_)\n",
        "\n",
        "######## 엑스트라 트리\n",
        "# 랜덤포레스트와 비슷\n",
        "# 부트스트랩 샘플을 사용하지 않고, 각 결정트리를 만들 때 전체 훈련 세트를 사용\n",
        "# 노드를 분할할 때, 최적을 찾지 않고 무작위로 감\n",
        "# --> 엑스트라 트리가 무작위성이 높기 때문에 더 많은 양을 훈련해야함 but 랜덤이라서 더 훈련속도는 빠름\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)\n",
        "\n",
        "#---------------------------------------------------------------------\n",
        "# 앙상블의 방법\n",
        "# --> 사진 볼것\n",
        "# 배깅 -> 병렬로\n",
        "# 부스팅 -> 순서대로\n",
        "\n",
        "# 그레디언트 부스팅\n",
        "# 깊이가 얕은 결정트리를 사용하여 이전트리의 오차를 보완하는 방식\n",
        "# 트리의 형태에서 경사하강법의 개념을 사용\n",
        "# -> 사진 보셈\n",
        "# 얕은 학습 -> 결정트리의 개수를 늘려도 과대적합에 매우 강함\n",
        "# -> 일반적으로 랜덤포레스트보다 성능이 더 높지만, 훈련시간이 오래걸림\n",
        "\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)\n",
        "\n",
        "\n",
        "# 히스토그램 기반 그레디언트 부스팅\n",
        "# 히스토그램? -> 데이터를 일정한 구간으로 쪽서 막대그래프로 표현한 것\n",
        "# 예를 들어 특성이 512개 있다면 -> 특성 1,2를 1로 특성 3,4를 2로 축소\n",
        "# 256번만 계산하면되므로 효율적\n",
        "# ==> 256개의 특성으로 줄임\n",
        "\n",
        "\n",
        "# 사이킷런 1.0 버전 아래에서는 다음 라인의 주석을 해제하고 실행하세요.\n",
        "# from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "\n",
        "# 기존의 방식들과 다르게 히스토그램 기반 그레디언트 부스팅은 permutation_importance을 통해 특성 중요도를 계산해서 산출해야한다.\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "\n",
        "\n",
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "\n",
        "hgb.score(test_input, test_target)\n",
        "\n",
        "# XGBoost\n",
        "#사이킷 런의 cross_validate() 함수 사용가능\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "\n",
        "#LightGBM\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    }
  ]
}